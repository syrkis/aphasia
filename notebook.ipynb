{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit\n",
    "import optax\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from string import ascii_letters, digits, punctuation, whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = ascii_letters + digits + punctuation + whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(n_books=100):\n",
    "    c2i = {c: i for i, c in enumerate(alphabet)}\n",
    "    i2c = {i: c for i, c in enumerate(alphabet)}\n",
    "    PATH = os.getcwd() + '/data/books'\n",
    "    files = [os.path.join(PATH, f) for f in os.listdir(PATH) if f.endswith('.txt')]\n",
    "    codex = []\n",
    "    for f in tqdm(files[:n_books]):\n",
    "        with open(f, 'r') as file:\n",
    "            txt = file.read().split('***')[2]\n",
    "            txt = [c2i[char] for char in txt if char in alphabet]\n",
    "            if len(txt) > 1000:\n",
    "                codex += txt\n",
    "    return codex, c2i, i2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(rng, data, batch_size, block_length):\n",
    "    while True:\n",
    "        # data is one looooooong string. Pick batch random starting points\n",
    "        idxs = jax.random.randint(rng, shape=(batch_size,), minval=0, maxval=len(data) - block_length - 1)\n",
    "        batch = jnp.array([data[idx:idx+block_length + 1] for idx in idxs])\n",
    "        yield batch[:, :-1], batch[:, 1:]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_fn(rng, n_chars, n_embed, n_hidden, scale=1e-2):\n",
    "    embedding = scale * jax.random.normal(rng, shape=(n_chars, n_embed))\n",
    "    fc1 = {\n",
    "        'w': scale * jax.random.normal(rng, shape=(n_embed, n_hidden)),\n",
    "        'b': scale * jax.random.normal(rng, shape=(n_hidden,))\n",
    "    }\n",
    "    fc2 = {\n",
    "        'w': scale * jax.random.normal(rng, shape=(n_hidden, n_chars)),\n",
    "        'b': scale * jax.random.normal(rng, shape=(n_chars,))\n",
    "    }\n",
    "    return {'embedding': embedding, 'fc': {'fc1': fc1, 'fc2': fc2}}\n",
    "\n",
    "def apply_fn(params, x):\n",
    "    x = x.reshape(-1)\n",
    "    z = params['embedding'][x]\n",
    "    z = jnp.tanh(jnp.dot(z, params['fc']['fc1']['w']) + params['fc']['fc1']['b'])\n",
    "    z = jnp.dot(z, params['fc']['fc2']['w']) + params['fc']['fc2']['b']\n",
    "    return z\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    y_pred = apply_fn(params, x)\n",
    "    log_probs = jax.nn.log_softmax(y_pred, axis=-1)\n",
    "    correct_log_probs = jnp.take_along_axis(log_probs, y.flatten()[:, None], axis=1)\n",
    "    correct_log_probs = jnp.nan_to_num(correct_log_probs, nan=-1e3)\n",
    "    loss = -jnp.mean(correct_log_probs)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "data, c2i, i2c = get_data() if 'data' not in globals() else eval('data, c2i, i2c')\n",
    "batches = get_batches(rng, data, 32, 128)\n",
    "params = init_fn(rng, len(alphabet), 64, 128)\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 762.0879\n",
      "Loss: 762.0870\n",
      "Loss: 762.0863\n",
      "Loss: 762.0854\n",
      "Loss: 762.0845\n",
      "Loss: 762.0835\n",
      "Loss: 762.0824\n",
      "Loss: 762.0812\n",
      "Loss: 762.0798\n",
      "Loss: 762.0782\n",
      "Loss: 762.0765\n",
      "Loss: 762.0746\n",
      "Loss: 762.0725\n",
      "Loss: 762.0701\n",
      "Loss: 762.0674\n",
      "Loss: 762.0645\n",
      "Loss: 762.0612\n",
      "Loss: 762.0576\n",
      "Loss: 762.0535\n",
      "Loss: 762.0491\n",
      "Loss: 762.0442\n",
      "Loss: 762.0389\n",
      "Loss: 762.0331\n",
      "Loss: 762.0267\n",
      "Loss: 762.0198\n",
      "Loss: 762.0123\n",
      "Loss: 762.0043\n",
      "Loss: 761.9957\n",
      "Loss: 761.9866\n",
      "Loss: 761.9769\n",
      "Loss: 761.9666\n",
      "Loss: 761.9558\n",
      "Loss: 761.9445\n",
      "Loss: 761.9328\n",
      "Loss: 761.9207\n",
      "Loss: 761.9083\n",
      "Loss: 761.8958\n",
      "Loss: 761.8831\n",
      "Loss: 761.8704\n",
      "Loss: 761.8578\n",
      "Loss: 761.8456\n",
      "Loss: 761.8337\n",
      "Loss: 761.8225\n",
      "Loss: 761.8120\n",
      "Loss: 761.8024\n",
      "Loss: 761.7937\n",
      "Loss: 761.7859\n",
      "Loss: 761.7792\n",
      "Loss: 761.7733\n",
      "Loss: 761.7684\n",
      "Loss: 761.7643\n",
      "Loss: 761.7609\n",
      "Loss: 761.7581\n",
      "Loss: 761.7555\n",
      "Loss: 761.7533\n",
      "Loss: 761.7512\n",
      "Loss: 761.7493\n",
      "Loss: 761.7474\n",
      "Loss: 761.7456\n",
      "Loss: 761.7438\n",
      "Loss: 761.7421\n",
      "Loss: 761.7405\n",
      "Loss: 761.7390\n",
      "Loss: 761.7375\n",
      "Loss: 761.7363\n",
      "Loss: 761.7352\n",
      "Loss: 761.7341\n",
      "Loss: 761.7333\n",
      "Loss: 761.7324\n",
      "Loss: 761.7317\n",
      "Loss: 761.7310\n",
      "Loss: 761.7303\n",
      "Loss: 761.7297\n",
      "Loss: 761.7290\n",
      "Loss: 761.7284\n",
      "Loss: 761.7277\n",
      "Loss: 761.7271\n",
      "Loss: 761.7264\n",
      "Loss: 761.7257\n",
      "Loss: 761.7251\n",
      "Loss: 761.7245\n",
      "Loss: 761.7240\n",
      "Loss: 761.7235\n",
      "Loss: 761.7230\n",
      "Loss: 761.7227\n",
      "Loss: 761.7222\n",
      "Loss: 761.7219\n",
      "Loss: 761.7216\n",
      "Loss: 761.7212\n",
      "Loss: 761.7209\n",
      "Loss: 761.7206\n",
      "Loss: 761.7203\n",
      "Loss: 761.7200\n",
      "Loss: 761.7197\n",
      "Loss: 761.7195\n",
      "Loss: 761.7192\n",
      "Loss: 761.7189\n",
      "Loss: 761.7186\n",
      "Loss: 761.7183\n",
      "Loss: 761.7180\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    x, y = next(batches)\n",
    "    loss = loss_fn(params, x, y)\n",
    "    print(f'Loss: {loss:.4f}')\n",
    "    grad_fn = jax.grad(loss_fn)\n",
    "    grads = grad_fn(params, x, y)\n",
    "    updates, opt_state = opt.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
